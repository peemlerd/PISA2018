{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note about project:  This project is exploratory. I plan to run regression on students' performance and their \n",
    "attitude towards education. Other factors aside from attitude, such as socioeconomic status, definitely affect \n",
    "education performance regardless of students' attitude. \n",
    "\n",
    "Goal of this project: Test which questions strongly correlate with performance to help with the future data collection \n",
    "by TDRI.  If we have a general idea on which specific attitudes correlate with education, we can design a survey \n",
    "that better captures the result of education interventions. \n",
    "\"\"\"\n",
    "# Importing all the necessary libraries. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: List of numbers\n",
    "Output: The average of members within the list.\n",
    "Description: This function helps when I apply conditions to the dataframe, such as separating students into those\n",
    "that attend public and private schools. In some instances, the length of the dataframe is 0, and dividing 0 gives\n",
    "an error. Because I prefer to abstract from checking the type of elements within each array, I create this function. \n",
    "\"\"\" \n",
    "\n",
    "def avg(lst):\n",
    "    count = 0\n",
    "    sum_val = 0\n",
    "    for member in lst:\n",
    "        try:\n",
    "            temp = float(member)\n",
    "            count += 1\n",
    "            sum_val += temp\n",
    "        except:\n",
    "            temp = 0\n",
    "    try:\n",
    "        return sum_val/count\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading stata file. Note that columns with all N/A cell will be dropped from this dataset. \n",
    "# I have experimented with dropping columns with any N/A cell, but the final result was too many dataset is dropped.\n",
    "\"\"\"\n",
    "Notes on data:\n",
    "1. This data is a survey administered to approximately 8600 students attending 290 different schools. Some of\n",
    "the columns contain categorical variables, and others ordinal/numerical variables. This suggests the need to carefully\n",
    "select which datafield will be included in the final analysis.\n",
    "2. Some of the cells contain No Response/Not available. I must decide later on how to impute/remove these data\n",
    "before running the regression model. \n",
    "\"\"\"\n",
    "\n",
    "thadf = pd.read_stata(\"PISA_2018_THA.dta\").dropna(how = \"all\", axis = 1).fillna(method = \"ffill\").fillna(method = \"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatinga a dictionary whose key is column name and value is column interpretation. \n",
    "# This method helps because when I tried reading .dta files directly, some of the values change. \n",
    "\"\"\"\n",
    "Credit: https://stackoverflow.com/questions/44809696/is-there-a-way-to-read-stata-labels-in-python\n",
    "\"\"\"\n",
    "\n",
    "reader = pd.io.stata.StataReader(\"PISA_2018_THA.dta\")\n",
    "header = reader.variable_labels()\n",
    "for var in header:\n",
    "        name  = var\n",
    "        label = header[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating the keys of columns we have already dropped so that we can take advantage of .columns.tolist() library.\n",
    "\n",
    "header_temp = {}\n",
    "for key in list(thadf.columns.tolist()):\n",
    "    header_temp[key] = header[key]\n",
    "\n",
    "header = header_temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a flipped dictionary so that we can search for the column name if we have an aspect of data we are \n",
    "# interested in. \n",
    "\n",
    "header_flip = {}\n",
    "for key in list(header.keys()):\n",
    "    header_flip[header[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating three new columns -- math, read, scie, glcm -- to calculate the average statistics for each student.  \n",
    "\"\"\"\n",
    "Note: Global competency (GLCM) is dropped because there is no clear way to calculate the competency. For instance,\n",
    "some questions have a lot of No Responses; others including questions that are not taught in Thai curriculum. \n",
    "Therefore, I focused on mathematics, science, and reading because these subjects are less context-dependent.\n",
    "\"\"\"\n",
    "\n",
    "num_row = thadf.shape[0]\n",
    "thadf = thadf.assign(math = [0]*num_row, read = [0]*num_row, scie = [0]*num_row) \n",
    "col_lst = thadf.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the summary statistics for mathematics and science by averaging scores for all 10 levels in the subject. \n",
    "\n",
    "subject_lst = [\"math\", \"scie\"]\n",
    "temp = \"pv\"\n",
    "for index in thadf.index.tolist():\n",
    "    for subject in subject_lst:\n",
    "        sum_score = 0\n",
    "        for i in range(1,11):\n",
    "            col_name = temp + str(i) + subject\n",
    "            sum_score += thadf.loc[index, col_name]\n",
    "        avg_score = sum_score/10\n",
    "        thadf.loc[index, subject] = avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the summary statistics for reading by averaging scores for all 10 levels across different areas of\n",
    "# reading. \n",
    "\"\"\"\n",
    "Each subfield of reading is as follows:\n",
    "1.) rtsn = Single Text Structure. \n",
    "2.) rcer = Evaluate and Reflect \n",
    "3.) Multiple Text Structure\n",
    "4.) rcun = Reading comprehenseion \n",
    "5.) rcli = Locate information  \n",
    "\n",
    "Note: This analysis could be more detailed. For instance, we could treat reading comprehension and locating \n",
    "information as separate fields of data. However, I chose to average their assessment score to understand a larger\n",
    "trend of Thai students' reading skills. \n",
    "\"\"\"\n",
    "read_lst = [\"rtsn\", \"rcer\",\"rtml\", \"rcun\", \"rcli\"]\n",
    "for index in thadf.index.tolist():\n",
    "    sum_score = 0\n",
    "    for read in read_lst:\n",
    "        for i in range(1,11):\n",
    "            col_name = temp + str(i) + read\n",
    "            sum_score += thadf.loc[index, col_name]\n",
    "    avg_score = sum_score/50 # Average over 50 types of reading\n",
    "    thadf.loc[index, \"read\"] = avg_score\n",
    "subject_lst = subject_lst + [\"read\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At this point, I will run a regression project on students' attitude.\n",
    "\n",
    "Note: Expected maximum education attainment has some idiosyncrasy. There are 6000+ students who check bachelor's\n",
    "degree, but do not check middle school and high school. Therefore, I chose to exclude this data from the analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Create a list of questions pertaining to attitude by grouping relevant questions altogether into \n",
    "# different groups. \n",
    "\n",
    "content = [\"st016q01na\"]\n",
    "school_try = [\"st036q05ta\",\"st036q06ta\",\"st036q08ta\"]\n",
    "max_edu = [\"st225q01ha\",\"st225q02ha\",\"st225q03ha\", \"st225q04ha\",\"st225q06ha\"]\n",
    "compete = [\"st181q02ha\", \"st181q03ha\", \"st181q04ha\"]\n",
    "failure = [\"st183q01ha\",\"st183q02ha\",\"st183q03ha\"]\n",
    "growth = [\"st184q01ha\"]\n",
    "life_meaning = [\"st185q01ha\",\"st185q02ha\",\"st185q03ha\"]\n",
    "mental_health = []\n",
    "for i in range(1,10):\n",
    "    temp = \"st186q0\"\n",
    "    if (i != 4):\n",
    "        temp += str(i) + \"ha\"\n",
    "        mental_health.append(temp)\n",
    "mental_health.append(\"st186q10ha\")\n",
    "mastery = [\"st208q01ha\",\"st208q02ha\",\"st208q04ha\"]\n",
    "self_manage = []\n",
    "for i in range(1,8):\n",
    "    temp = \"st188q0\"\n",
    "    if (i != 5 and i != 4):\n",
    "        temp += str(i) + \"ha\"\n",
    "        self_manage.append(temp)\n",
    "school_exp = []\n",
    "for i in range(1,7):\n",
    "    temp = \"st034q0\"\n",
    "    if (True):\n",
    "        temp += str(i) + \"ta\"\n",
    "        school_exp.append(temp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging each list of question groups altogether. \n",
    "attdf_questions = content + school_try + max_edu + compete + failure + growth + life_meaning + mental_health + mastery\n",
    "attdf_questions += self_manage + school_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a dictionary where we check how many misses/no responses are there in each variable.\n",
    "\"\"\"\n",
    "Note: no_resp_Qkey is a dictionary whose key is the question_id and value is the number of total 'No Response' from \n",
    "every student. no_resp_Skey is a dictionary whose key is the student_id and value the number of 'No Response' that\n",
    "student provides in total. \n",
    "\"\"\"\n",
    "\n",
    "attdf = thadf[attdf_questions]\n",
    "\n",
    "no_resp_Qkey = {} \n",
    "no_resp_Skey = {}\n",
    "\n",
    "for question in attdf_questions:\n",
    "    no_respdf = attdf[attdf[question] == \"No Response\"]\n",
    "    no_resp_Qkey[question] = no_respdf.shape[0]\n",
    "\n",
    "for index in attdf.index.tolist(): \n",
    "    no_respdf = attdf.loc[index]\n",
    "    for question in attdf_questions:\n",
    "        try:\n",
    "            no_resp_Skey[index] += 1\n",
    "        except:\n",
    "            no_resp_Skey[index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8633\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "After checking the no_resp_Qkey, I decided not to drop any questions because questions which have the highest\n",
    "'No Response' perform pretty well with approximate 250 misses out of the total 8600 responses. Therefore, I will\n",
    "drop students who provide a lot of 'No Response' instead, which in this case, I chose to drop any student with\n",
    "more than 1 'No Response'.\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "temp = 2\n",
    "for mem in list(no_resp_Skey.values()):\n",
    "    if mem >= temp:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a new dictionary for each student whereby agree/disagree is converted to 1-4 scale where\n",
    "# 1 = Strongly disagree/Never; 2 = Disagree/Rarely; 3 = Agree/Sometimes; 4 = Strongly agree/Always.\n",
    "\n",
    "\"\"\"\n",
    "Note: Because responses to each question is ordinal (1-4/1-5 scale on disagree-strongly agree), I created a new dataframe \n",
    "to run linear regression. I specifically used the to_dict method to take advantage of the fast computation\n",
    "of the to_list method on each column. To deal with 'No Response', I chose to impute the cell with the responses \n",
    "that appear the most commonly among the responses given. Because there are not many 'No Response' in the first place,\n",
    "this method would not significantly affect the final result of the regression model. \n",
    "\"\"\"\n",
    "\n",
    "data = {}\n",
    "# I created an expected_edu_lst to exclude these responses from the regression model. The reason is that some of the \n",
    "expected_edu_lst = [\"st225q01ha\",\"st225q02ha\",\"st225q03ha\",\"st225q04ha\",\"st225q06ha\"]\n",
    "\n",
    "for question in attdf_questions: \n",
    "    # This question 'st016q01na' is a numerical variable., so we can directly copy the value into a new array \n",
    "    # or impute. \n",
    "    if question == 'st016q01na':\n",
    "        temp_lst = []\n",
    "        for response in attdf[question].tolist():\n",
    "            # Directly adding the value to the list if the student provides some response.\n",
    "            if response != \"No Response\":\n",
    "                temp_lst.append(response)\n",
    "            else:\n",
    "            # Else, impute the No Response with the most common answers that appeared in the column.  \n",
    "                most_common = attdf[question].value_counts().index.tolist()[0]\n",
    "                temp_lst.append(most_common)\n",
    "    # If the question is ordinal variable, we need to convert into 1-4/1-5 scale first. \n",
    "    elif question not in subject_lst and question not in expected_edu_lst:\n",
    "        temp_lst = []\n",
    "        for response in attdf[question].tolist():\n",
    "            if response == \"No Response\":\n",
    "                #  Impute the No Response with the most common answers that appeared in the column.  \n",
    "                response = attdf[question].value_counts().index.tolist()[0]\n",
    "            if response in [\"Strongly disagree\",\"Never\",\"Not at all true of me\"]:\n",
    "                temp_lst.append(1)\n",
    "            elif response in [\"Disagree\", \"Rarely\", \"Slightly true of me\"]:\n",
    "                temp_lst.append(2)\n",
    "            elif response in [\"Agree\", \"Sometimes\", \"Moderately true of me\"]:\n",
    "                temp_lst.append(3)\n",
    "            elif response in [\"Strongly agree\", \"Always\",\"Very true of me\"]:\n",
    "                temp_lst.append(4) \n",
    "            elif response == \"Extremely true of me\":\n",
    "                temp_lst.append(5)\n",
    "    data[question] = temp_lst\n",
    "\n",
    "for subject in subject_lst:\n",
    "    data[subject] = thadf[subject].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe where every attitude-related question is converted into numbers. \n",
    "replace_attdf = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the list of expected level of education \n",
    "replace_attdf.drop(expected_edu_lst, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lst = []\n",
    "for question in attdf_questions:\n",
    "    if question not in expected_edu_lst and question not in subject_lst:\n",
    "        temp_lst.append(question)\n",
    "attdf_questions = temp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>read</td>       <th>  R-squared:         </th> <td>   0.384</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.381</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   144.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 26 Jan 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:38:35</td>     <th>  Log-Likelihood:    </th> <td> -48959.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  8633</td>      <th>  AIC:               </th> <td>9.799e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  8595</td>      <th>  BIC:               </th> <td>9.826e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    37</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>  346.5665</td> <td>   10.400</td> <td>   33.324</td> <td> 0.000</td> <td>  326.180</td> <td>  366.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st016q01na</th> <td>   -3.5188</td> <td>    0.388</td> <td>   -9.066</td> <td> 0.000</td> <td>   -4.280</td> <td>   -2.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st036q05ta</th> <td>  -12.8438</td> <td>    1.751</td> <td>   -7.335</td> <td> 0.000</td> <td>  -16.276</td> <td>   -9.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st036q06ta</th> <td>   12.3844</td> <td>    1.754</td> <td>    7.061</td> <td> 0.000</td> <td>    8.946</td> <td>   15.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st036q08ta</th> <td>    2.5028</td> <td>    1.738</td> <td>    1.440</td> <td> 0.150</td> <td>   -0.905</td> <td>    5.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st181q02ha</th> <td>  -14.9442</td> <td>    1.186</td> <td>  -12.598</td> <td> 0.000</td> <td>  -17.270</td> <td>  -12.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st181q03ha</th> <td>   -8.1848</td> <td>    1.278</td> <td>   -6.405</td> <td> 0.000</td> <td>  -10.690</td> <td>   -5.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st181q04ha</th> <td>   20.9648</td> <td>    1.348</td> <td>   15.556</td> <td> 0.000</td> <td>   18.323</td> <td>   23.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st183q01ha</th> <td>    8.5158</td> <td>    1.379</td> <td>    6.176</td> <td> 0.000</td> <td>    5.813</td> <td>   11.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st183q02ha</th> <td>   10.4658</td> <td>    1.459</td> <td>    7.171</td> <td> 0.000</td> <td>    7.605</td> <td>   13.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st183q03ha</th> <td>   -9.1129</td> <td>    1.233</td> <td>   -7.393</td> <td> 0.000</td> <td>  -11.529</td> <td>   -6.697</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st184q01ha</th> <td>  -22.9369</td> <td>    0.879</td> <td>  -26.108</td> <td> 0.000</td> <td>  -24.659</td> <td>  -21.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st185q01ha</th> <td>   -6.5256</td> <td>    1.624</td> <td>   -4.017</td> <td> 0.000</td> <td>   -9.710</td> <td>   -3.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st185q02ha</th> <td>   -8.2490</td> <td>    1.738</td> <td>   -4.746</td> <td> 0.000</td> <td>  -11.656</td> <td>   -4.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st185q03ha</th> <td>   -0.9889</td> <td>    1.706</td> <td>   -0.580</td> <td> 0.562</td> <td>   -4.333</td> <td>    2.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q01ha</th> <td>   -1.7172</td> <td>    1.622</td> <td>   -1.059</td> <td> 0.290</td> <td>   -4.896</td> <td>    1.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q02ha</th> <td>   21.5135</td> <td>    1.385</td> <td>   15.530</td> <td> 0.000</td> <td>   18.798</td> <td>   24.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q03ha</th> <td>   -1.0887</td> <td>    1.700</td> <td>   -0.641</td> <td> 0.522</td> <td>   -4.420</td> <td>    2.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q05ha</th> <td>   19.4841</td> <td>    1.684</td> <td>   11.569</td> <td> 0.000</td> <td>   16.183</td> <td>   22.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q06ha</th> <td>   -3.1652</td> <td>    1.217</td> <td>   -2.600</td> <td> 0.009</td> <td>   -5.552</td> <td>   -0.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q07ha</th> <td>    4.5799</td> <td>    1.638</td> <td>    2.796</td> <td> 0.005</td> <td>    1.369</td> <td>    7.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q08ha</th> <td>   -8.0557</td> <td>    1.429</td> <td>   -5.638</td> <td> 0.000</td> <td>  -10.857</td> <td>   -5.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q09ha</th> <td>  -13.7077</td> <td>    1.370</td> <td>  -10.005</td> <td> 0.000</td> <td>  -16.393</td> <td>  -11.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q10ha</th> <td>   -1.2315</td> <td>    1.453</td> <td>   -0.848</td> <td> 0.397</td> <td>   -4.079</td> <td>    1.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st208q01ha</th> <td>   -0.6677</td> <td>    1.245</td> <td>   -0.536</td> <td> 0.592</td> <td>   -3.108</td> <td>    1.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st208q02ha</th> <td>    2.8251</td> <td>    1.424</td> <td>    1.984</td> <td> 0.047</td> <td>    0.034</td> <td>    5.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st208q04ha</th> <td>    5.4683</td> <td>    1.403</td> <td>    3.897</td> <td> 0.000</td> <td>    2.718</td> <td>    8.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st188q01ha</th> <td>   11.5269</td> <td>    1.799</td> <td>    6.409</td> <td> 0.000</td> <td>    8.001</td> <td>   15.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st188q02ha</th> <td>   27.8490</td> <td>    1.651</td> <td>   16.863</td> <td> 0.000</td> <td>   24.612</td> <td>   31.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st188q03ha</th> <td>   -4.0951</td> <td>    1.300</td> <td>   -3.151</td> <td> 0.002</td> <td>   -6.643</td> <td>   -1.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st188q06ha</th> <td>    8.8472</td> <td>    1.708</td> <td>    5.180</td> <td> 0.000</td> <td>    5.499</td> <td>   12.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st188q07ha</th> <td>  -11.1366</td> <td>    1.691</td> <td>   -6.585</td> <td> 0.000</td> <td>  -14.452</td> <td>   -7.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st034q01ta</th> <td>   -6.5269</td> <td>    1.332</td> <td>   -4.900</td> <td> 0.000</td> <td>   -9.138</td> <td>   -3.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st034q02ta</th> <td>   -2.7764</td> <td>    1.395</td> <td>   -1.991</td> <td> 0.047</td> <td>   -5.511</td> <td>   -0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st034q03ta</th> <td>    4.3039</td> <td>    1.403</td> <td>    3.067</td> <td> 0.002</td> <td>    1.553</td> <td>    7.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st034q04ta</th> <td>   -9.8772</td> <td>    1.194</td> <td>   -8.274</td> <td> 0.000</td> <td>  -12.217</td> <td>   -7.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st034q05ta</th> <td>    2.3692</td> <td>    1.303</td> <td>    1.818</td> <td> 0.069</td> <td>   -0.185</td> <td>    4.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st034q06ta</th> <td>   -4.2059</td> <td>    1.323</td> <td>   -3.179</td> <td> 0.001</td> <td>   -6.800</td> <td>   -1.612</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>80.048</td> <th>  Durbin-Watson:     </th> <td>   1.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  82.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.231</td> <th>  Prob(JB):          </th> <td>1.35e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.123</td> <th>  Cond. No.          </th> <td>    269.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   read   R-squared:                       0.384\n",
       "Model:                            OLS   Adj. R-squared:                  0.381\n",
       "Method:                 Least Squares   F-statistic:                     144.7\n",
       "Date:                Sun, 26 Jan 2020   Prob (F-statistic):               0.00\n",
       "Time:                        15:38:35   Log-Likelihood:                -48959.\n",
       "No. Observations:                8633   AIC:                         9.799e+04\n",
       "Df Residuals:                    8595   BIC:                         9.826e+04\n",
       "Df Model:                          37                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        346.5665     10.400     33.324      0.000     326.180     366.953\n",
       "st016q01na    -3.5188      0.388     -9.066      0.000      -4.280      -2.758\n",
       "st036q05ta   -12.8438      1.751     -7.335      0.000     -16.276      -9.411\n",
       "st036q06ta    12.3844      1.754      7.061      0.000       8.946      15.823\n",
       "st036q08ta     2.5028      1.738      1.440      0.150      -0.905       5.911\n",
       "st181q02ha   -14.9442      1.186    -12.598      0.000     -17.270     -12.619\n",
       "st181q03ha    -8.1848      1.278     -6.405      0.000     -10.690      -5.680\n",
       "st181q04ha    20.9648      1.348     15.556      0.000      18.323      23.607\n",
       "st183q01ha     8.5158      1.379      6.176      0.000       5.813      11.219\n",
       "st183q02ha    10.4658      1.459      7.171      0.000       7.605      13.327\n",
       "st183q03ha    -9.1129      1.233     -7.393      0.000     -11.529      -6.697\n",
       "st184q01ha   -22.9369      0.879    -26.108      0.000     -24.659     -21.215\n",
       "st185q01ha    -6.5256      1.624     -4.017      0.000      -9.710      -3.341\n",
       "st185q02ha    -8.2490      1.738     -4.746      0.000     -11.656      -4.842\n",
       "st185q03ha    -0.9889      1.706     -0.580      0.562      -4.333       2.355\n",
       "st186q01ha    -1.7172      1.622     -1.059      0.290      -4.896       1.461\n",
       "st186q02ha    21.5135      1.385     15.530      0.000      18.798      24.229\n",
       "st186q03ha    -1.0887      1.700     -0.641      0.522      -4.420       2.243\n",
       "st186q05ha    19.4841      1.684     11.569      0.000      16.183      22.785\n",
       "st186q06ha    -3.1652      1.217     -2.600      0.009      -5.552      -0.779\n",
       "st186q07ha     4.5799      1.638      2.796      0.005       1.369       7.791\n",
       "st186q08ha    -8.0557      1.429     -5.638      0.000     -10.857      -5.255\n",
       "st186q09ha   -13.7077      1.370    -10.005      0.000     -16.393     -11.022\n",
       "st186q10ha    -1.2315      1.453     -0.848      0.397      -4.079       1.616\n",
       "st208q01ha    -0.6677      1.245     -0.536      0.592      -3.108       1.772\n",
       "st208q02ha     2.8251      1.424      1.984      0.047       0.034       5.617\n",
       "st208q04ha     5.4683      1.403      3.897      0.000       2.718       8.219\n",
       "st188q01ha    11.5269      1.799      6.409      0.000       8.001      15.053\n",
       "st188q02ha    27.8490      1.651     16.863      0.000      24.612      31.086\n",
       "st188q03ha    -4.0951      1.300     -3.151      0.002      -6.643      -1.547\n",
       "st188q06ha     8.8472      1.708      5.180      0.000       5.499      12.195\n",
       "st188q07ha   -11.1366      1.691     -6.585      0.000     -14.452      -7.821\n",
       "st034q01ta    -6.5269      1.332     -4.900      0.000      -9.138      -3.916\n",
       "st034q02ta    -2.7764      1.395     -1.991      0.047      -5.511      -0.042\n",
       "st034q03ta     4.3039      1.403      3.067      0.002       1.553       7.054\n",
       "st034q04ta    -9.8772      1.194     -8.274      0.000     -12.217      -7.537\n",
       "st034q05ta     2.3692      1.303      1.818      0.069      -0.185       4.923\n",
       "st034q06ta    -4.2059      1.323     -3.179      0.001      -6.800      -1.612\n",
       "==============================================================================\n",
       "Omnibus:                       80.048   Durbin-Watson:                   1.278\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               82.298\n",
       "Skew:                           0.231   Prob(JB):                     1.35e-18\n",
       "Kurtosis:                       3.123   Cond. No.                         269.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Replace ordinal variables on disagree-agree. \n",
    "# Result: Running the regression model with all variables yields an R-squared of 0.317. \n",
    "\"\"\"\n",
    "Note: The constant is added because every student should have a based intelligent even before entering school. \n",
    "\"\"\"\n",
    "\n",
    "X = replace_attdf[attdf_questions]\n",
    "y = replace_attdf[\"read\"]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>read</td>       <th>  R-squared:         </th> <td>   0.323</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.322</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   410.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 26 Jan 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:38:06</td>     <th>  Log-Likelihood:    </th> <td> -49368.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  8633</td>      <th>  AIC:               </th> <td>9.876e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  8622</td>      <th>  BIC:               </th> <td>9.883e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>  246.0985</td> <td>    8.836</td> <td>   27.851</td> <td> 0.000</td> <td>  228.777</td> <td>  263.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st036q06ta</th> <td>    4.5318</td> <td>    1.227</td> <td>    3.693</td> <td> 0.000</td> <td>    2.126</td> <td>    6.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st181q02ha</th> <td>  -21.3093</td> <td>    1.141</td> <td>  -18.679</td> <td> 0.000</td> <td>  -23.546</td> <td>  -19.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st181q04ha</th> <td>   19.4202</td> <td>    1.312</td> <td>   14.803</td> <td> 0.000</td> <td>   16.848</td> <td>   21.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st183q02ha</th> <td>   12.4425</td> <td>    1.096</td> <td>   11.348</td> <td> 0.000</td> <td>   10.293</td> <td>   14.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st184q01ha</th> <td>  -26.3940</td> <td>    0.904</td> <td>  -29.211</td> <td> 0.000</td> <td>  -28.165</td> <td>  -24.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q02ha</th> <td>   18.3660</td> <td>    1.099</td> <td>   16.708</td> <td> 0.000</td> <td>   16.211</td> <td>   20.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st186q05ha</th> <td>   12.4283</td> <td>    1.303</td> <td>    9.541</td> <td> 0.000</td> <td>    9.875</td> <td>   14.982</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st208q04ha</th> <td>    3.5666</td> <td>    0.953</td> <td>    3.743</td> <td> 0.000</td> <td>    1.698</td> <td>    5.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st188q02ha</th> <td>   30.6935</td> <td>    1.548</td> <td>   19.834</td> <td> 0.000</td> <td>   27.660</td> <td>   33.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>st034q01ta</th> <td>  -12.9893</td> <td>    1.108</td> <td>  -11.726</td> <td> 0.000</td> <td>  -15.161</td> <td>  -10.818</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>145.740</td> <th>  Durbin-Watson:     </th> <td>   1.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 152.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.322</td>  <th>  Prob(JB):          </th> <td>6.60e-34</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.097</td>  <th>  Cond. No.          </th> <td>    107.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   read   R-squared:                       0.323\n",
       "Model:                            OLS   Adj. R-squared:                  0.322\n",
       "Method:                 Least Squares   F-statistic:                     410.7\n",
       "Date:                Sun, 26 Jan 2020   Prob (F-statistic):               0.00\n",
       "Time:                        15:38:06   Log-Likelihood:                -49368.\n",
       "No. Observations:                8633   AIC:                         9.876e+04\n",
       "Df Residuals:                    8622   BIC:                         9.883e+04\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        246.0985      8.836     27.851      0.000     228.777     263.420\n",
       "st036q06ta     4.5318      1.227      3.693      0.000       2.126       6.937\n",
       "st181q02ha   -21.3093      1.141    -18.679      0.000     -23.546     -19.073\n",
       "st181q04ha    19.4202      1.312     14.803      0.000      16.848      21.992\n",
       "st183q02ha    12.4425      1.096     11.348      0.000      10.293      14.592\n",
       "st184q01ha   -26.3940      0.904    -29.211      0.000     -28.165     -24.623\n",
       "st186q02ha    18.3660      1.099     16.708      0.000      16.211      20.521\n",
       "st186q05ha    12.4283      1.303      9.541      0.000       9.875      14.982\n",
       "st208q04ha     3.5666      0.953      3.743      0.000       1.698       5.435\n",
       "st188q02ha    30.6935      1.548     19.834      0.000      27.660      33.727\n",
       "st034q01ta   -12.9893      1.108    -11.726      0.000     -15.161     -10.818\n",
       "==============================================================================\n",
       "Omnibus:                      145.740   Durbin-Watson:                   1.204\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              152.803\n",
       "Skew:                           0.322   Prob(JB):                     6.60e-34\n",
       "Kurtosis:                       3.097   Cond. No.                         107.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Choosing which variables to include in the final model.\n",
    "keep_lst = []\n",
    "# st036q05ta yields R-squared of only 0.001. st036q06ta (entering good university) yields the highest coefficient\n",
    "# and highest R-squared\n",
    "motivation = [\"st036q06ta\"]\n",
    "# st181q02ha = Enjoyment during competition (-); st181q04ha = Strive to do better in face of competition\n",
    "compete = [\"st181q02ha\",\"st181q04ha\"]\n",
    "fear_of_failure = [\"st183q02ha\"]\n",
    "growth_mindset = [\"st184q01ha\"] # Super important: R-squared of 0.108\n",
    "anxiety = [\"st186q02ha\"] \n",
    "happy = [\"st186q05ha\"]\n",
    "# Unhappy emotion (i.e. \"st186q 06/08/10 ha\" has insignificant R-squared)\n",
    "learning_goal = [\"st208q04ha\"]\n",
    "accomplishment = [\"st188q02ha\"] # st188q02ha = I feel content when I can accomplish something. \n",
    "negative_school_exp = [\"st034q01ta\"] \n",
    "# Note: Positive school experience does not add much, but negative school experience means a lot\n",
    "\n",
    "\n",
    "keep_lst = motivation + compete + fear_of_failure + growth_mindset + anxiety + happy + learning_goal\n",
    "keep_lst += accomplishment + negative_school_exp\n",
    "\n",
    "X = replace_attdf[keep_lst]\n",
    "y = replace_attdf[\"read\"]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive factors are as follows: \n",
      "\n",
      "Thinking about your school: Trying hard at school will help me get into a good <\n",
      "Agree: I try harder when I’m in competition with other people.\n",
      "Agree: When I am failing, I am afraid that I might not have enough talent.\n",
      "Thinking about yourself and how you normally feel: how often do you feel as desc\n",
      "Thinking about yourself and how you normally feel: how often do you feel as desc\n",
      "How true for you: My goal is to understand the content of my classes as thorough\n",
      "Agree: I feel proud that I have accomplished things.\n",
      "\n",
      "Negative factors are as follows: \n",
      "\n",
      "Agree: I enjoy working in situations involving competition with others.\n",
      "Agree: Your intelligence is something about you that you can't change very much.\n",
      "Thinking about your school: I feel like an outsider (or left out of things) at s\n",
      "\n",
      "Strong factors are as follows: \n",
      "\n",
      "Thinking about your school: I feel like an outsider (or left out of things) at s\n",
      "Agree: I feel proud that I have accomplished things.\n",
      "Agree: Your intelligence is something about you that you can't change very much.\n",
      "Agree: I enjoy working in situations involving competition with others.\n",
      "Agree: I try harder when I’m in competition with other people.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Summarize results\n",
    "# Summary of methodology and result. \n",
    "\"\"\"\n",
    "Methodology: \n",
    "First, I loaded the PISA 2018 data on Thai students' education achievement. Because people have previously studied \n",
    "the impact of students' socioeconomic background on their education achievement, I decided to focus on questions\n",
    "pertaining to students' attitude towards education (referring to P.47-P.60 of the student questionnaire). \n",
    "\n",
    "Second, I looked at the data and observed that virtually every question is on an Agree/Disagree scale except for \n",
    "their expected education achievement. After looking at the expected education level, I concluded that the data \n",
    "was not properly filled since it added up to more than 8600 (the total number of students participating in this\n",
    "study). Therefore, I decided to remove it for the regression analysis. Moreover, we experienced several problems\n",
    "with No Response, so we filter out every student whose responses miss at least 2 spots. Not much data is missed \n",
    "given that only 227 out of 8700 students have at least 2 spots missed; for others, their data is complete or \n",
    "misses only one spot. \n",
    "\n",
    "Third, I converted the Agree/Disagree questionnaire into 1-4 scale and ran multiple linear regression. The result\n",
    "will be shown above. Only the variables with p-value less than 0.05 will be shown. \n",
    "\n",
    "Fourth, group the variables by positive/negative coefficient and pick only the strongest coefficients. \n",
    "That is, pick variables that have high explanatory power.\n",
    "\n",
    "Limitation: Reverse causality is not handled. \n",
    "\"\"\"\n",
    "\n",
    "# Summarizing factors that negatively and positively correlate with education achievement.\n",
    "negative_factor = [\"st181q02ha\",\"st184q01ha\",\"st034q01ta\"]\n",
    "positive_factor = [member for member in keep_lst if member not in negative_factor]\n",
    "\n",
    "# Selecting the factors that strongly associate with education achievement. \n",
    "strong_factor = [\"st034q01ta\",\"st188q02ha\",\"st184q01ha\",\"st181q02ha\",\"st181q04ha\"] \n",
    "\n",
    "# Displaying result\n",
    "print(\"Positive factors are as follows: \\n\")\n",
    "for factor in positive_factor:\n",
    "    print(header[factor])\n",
    "    \n",
    "print(\"\\nNegative factors are as follows: \\n\")\n",
    "for factor in negative_factor:\n",
    "    print(header[factor])\n",
    "    \n",
    "print(\"\\nStrong factors are as follows: \\n\")\n",
    "for factor in strong_factor:\n",
    "    print(header[factor])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
